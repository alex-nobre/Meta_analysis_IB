---
title: "Analysis notebook"
output: html_notebook
bibliography: library.bib
csl: apa.csl
---

This notebook details steps and decisions for the analysis of quantitive data in the meta-analysis.

Legends:

IB = inattentional blindness;
US = unexpected stimulus/stimuli;


# Study selection

The final sample for the qualitative review consisted of 27 papers. From those, the following papers were removed because the results were not behavioral:

* Harris et al. (2018)
* Schelonka et al. (2016)
* Pammer and blink (2018)
* Vandenbroucke et al. (2014)
* Wiemer et al. (2013)
* Scholte et al. (2006)
* Shafto and Pitts (2015)
* Pitts et al. (2011)
* Richards et al. (2012)

Additionally, another paper was removed for not reporting enough data to compute a summary effect size (Lathrop et al., 2011).

The remaining papers broach 49 experiments/contrasts. Some of those included more than one measure for each comparison (usually RT and accuracy). Those were included as a single line, and modelled with a three-level meta-analysis model.



# Variables

## us_relevance

Did the us interact with the target stimulus? This means the US interacted with the stimuli in the main task either by being part of it of by forming a configuration with it (e.g., the Ponzo illusion). US were classified as relevant even if they were not mentioned in the instructions.

Type: dichotomous variable. Levels: "relevant" or "irrelevant".


## gestalt

Did the study investigate the ocurrence of some king of gestalt processing (grouping, figure-ground segregation, etc.)?

Type: dichotomous variable. Levels: "yes" or "no".


## Is the implicit process perceptual or response-end?

This is not clear-cut, as the rationale of some tasks, e.g., the inattention paradigms, is not well-specified.

Type: dichotomous variable. Levels: ??


## implicit_measure

Is implicit processing measured by RT or accuracy?

Type: dichotomous variable. Levels: "RT" or "acc".


## N_trials_implicit

How many trials were used to assess implicit processing?

Type: integer variable. 


## N_participants_implicit

How many subjects in the sample for implicit processing testing?

Type: integer variable. 

This is either:

* Total N if group assessment of awareness was performed
* N of IB subjects if post-hoc data selection was performed


## N_trials_awareness

In how many trials was awareness assessed (how many assessments)? 

The number of assessments is the number of times the same type of assessment (e.g., yes or no question, forced-choice question) was performed, NOT the number of all assessments by all tasks. They do not group because they differ in sensitivity. Instead, we choose one of them as the assessment. It should be the more liberal one, so as not to underestimate awareness.

Type: integer variable.


## N_participants_awareness

How many subjects in the sample for awareness testing?

This is either:

* Total N if group assessment of awareness was performed
* N of IB subjects if post-hoc data selection was performed

Type: integer variable.


Manual computations for 

Pugnaghi et al. (2019): 

* Exp. 1: 80 - 1 (anticipated US) -  = 65
* Exp. 2: 80 - 3 (anticipated US) - 3 (too many errors) - 1 (error in data recording) = 75.
* Exp. 3: 80 - 1 (anticipated US) - 1 (exclusion criteria) - 6 (too many errors) = 72.
* Total = 212

Pugnaghi et al. (2020): 

* Exp. 1: 80 - 4 (full attention failure) - 1 (error in data recording) = 75
* Exp. 2: 114 - 2 (anticipated US) - 4 (full attention failure) - 2 (error in data recording) = 106

Kreitz et al. (2020) papers:

* Dataset 1: 123 - 2 (anticipated US) - 4 (full attention failure) - 1 (problems in thresholding procedure) = 116.
* Dataset 2 (see "legend_study_2.docx"): 200 - 2 (problems with instructions) - 2 (no matching conditions) - 4 (full attention failure in one of the tasks) = 192.
* Dataset 3: 120 - 2 (impaired vision) - 3 (full attention failure) - 4 (error in data recording) = 111.
* Dataset 4: 120 - 4 (impaired vision) - 4 (full attention failure) - 1 (anticipated US) = 110.
* Dataset 5: 120 - 3 (impaired vision) - 6 (full attention failure) - 7 (anticipated US) = 110.
* Dataset 6: 1104 - 550 (all exclusion criteria) = 554.
* Dataset 7: 100 - 5 (anticipated US) = 95.
* Dataset 8: 100 - 9 (anticipated US) - 1 (full attention failure) = 90.
* Dataset 9: 100 - 2 (impaired vision) - 9 (anticipated US) = 89.
* Dataset 10: 120 - 5 (all exclusion criteria) = 115.
* Dataset 13: 200 - 7 (anticipated US) - 4 (full attention failure) - 1 (error in data recording) = 188.
* Dataset 14: 200 - 7 (anticipated US) - 5 (full attention failure) - 3 (too many errors) - 1 (error in data recording) = 184.
* Dataset 15: 300 - 7 (anticipated US) - 5 (impaired vision) - 9 (full attention failure) = 277.
* Dataset 16: 300 - 20 (anticipated US) - 5 (knew the paradigm) - 5 (impaired vision) - 1 (incomplete experiment) - 9 (full attention failure) = 260.



##  Which type of awareness measure was employed (objective or subjective)?

Type: categorial variable. Levels: "objective", "subjective" or "objective/subjective".

* Objective measures: yes-or-no questions, forced-choice recognition tests.
* Subjective measures: perceptual awareness scale, confidence ratings, post-wagering scales.



## Was the US presented in a separate block/phase, or interleaved with non-US trials?

This is used to test the hypothesis that assessment of awareness after a whole block has a higher chance of mixing aware and unaware trials, due to memory issues.

Type: dichotomous variable. Levels: "block" or "interleaved".


## Was assessment of awareness of the US presence based on a trial or a block of trials?

Type: dichotomous variable. Levels: "trial" or "block".


## Was the result for the implicit test significant?

Type: dichotomous (categorical) variable. Levels: "yes" or "no".



## Gray literature

Was the contrast published in the "gray literature" (e.g., book chapter, conference abstract) instead of as journal article?

Type: dichotomous (categorical) variable. Levels: "yes" or "no".


## Static vs dynamic
 
Did the study investigate processing of static or moving US?

Type: dichotomous (categorical) variable. Levels: "static" or "dynamic".



# Effect size computations

Each experiment can contribute with one or more effect sizes/tests, one in each line. Each one of those is called a contrast.

We compute effect sizes based on data available in the papers and data sent by researchers. From the papers included after full-text reading, the following did not present enough data to allow the computation of an effect-size:
 
* Lathrop et al. (2011)
* Scholte et al. (2006) - MEG

Some experiments also did not provide sufficient data for computation of effect sizes for either RT or accuracy:

* list

These were considered only in the qualitative, but not in the quantitative analysis.

## Study designs

Studies are coded as between or within, to allow for correction according to Morris and DeShon [-@Morris2002].


## Direction of effect sizes

Effect sizes need a direction: if only the absolute value of the difference is recorded, all data points in the funnel plot will necessarily appear in the right side of the plot.

In our analysis, since there is no treatment/control group, we have to choose an experimental condition and a control condition to decide the direction. Our options are:

* When a study mentions a critical trial and a control trial, those are the experimental and the control conditions, respectively;
* When the above is not mentioned, but it is clear that in one condition there is an US, and the other does not, the first is the experimental condition, the second is the control;
* When a configuration is the US (e.g., grouping), the condition with the configuration is the experimental condition, the other is the control condition

Problem:in some cases, smaller RTs result from implicit processing (e.g., orienting of attention by arrows - Gabay); in others, larger RTs result (e.g., amodal completion of dashed lines - Moore and Egeth, 2003).  So we cannot simply subtract experimental - control, because this will result in effects in which mean same thing, but have opposite directions.

For this reason, we decided to code effects as positive if they were in the direction predicted by the hypothesis if the US was implicitly processed, and negative if they went in the opposite direction. We adopted the following conventions: 

* For continuous outcomes: We code effects in the direction of US processing (according to the direction predicted by the hypothesis) as positive and effects in the direction of no US processing (these will most likely be nonsignificant, but what matters is their contribution for the meta-analytic effect size) as negative.

* For comparisons against chance, deviations above chance mean a difference in the direction of US processing (according to the direction predicted by the hypothesis), thus positive; below chance (also in this case, most likely nonsignificant) as no US processing, thus negative.

- For interactions: when the interaction between V1 and V2 does not indicate a reversal in the direction of the effect of levels of V2 across levels of V1, the direction of the effect shown by the interaction is the same as the direction of the effect of V2.

- For congruence effects in inattention designs: a reversal of direction is expected. The direction is computed as below, following Jaccard (1998, p. 36):


Accuracy as error rate/RT (all but Rashal et al., 2017).

Smaller quantities indicate better performance. Plots are always shown with background "nested" (for display: the design in crossed) within target. Differences between differences are computed like (by convention):

(stsb - stdb) - (dtsb - dtdb)

if value is negative: interaction as predicted (ES is positive)
if value is positive: interaction is opposite of predicted (ES is negative)


Accuracy as percent of hits (Rashal et al., 2017).

Smaller quantities indicate better performance. Plots are always shown with background "nested" (for display: the design in crossed) within target. Differences between differences are computed like (by convention):

(stsb - stdb) - (dtsb - dtdb)

if value is positive: interaction as predicted (ES is positive)
if value is negative: interaction is opposite of predicted (ES is negative)


After computing r and Cohen's d, we chose to use hedge's g as standardized effect size. We computed hedge's g using formulas for matched groups from Borenstein's introduction to meta-analysis [@Borenstein2009], pp. 25-30.


## Formulas for computations of ES from specific statistics

### Proportions or percentages

When an implicit effect is computed from proportions or percentage, effect sizes are computed from the chi-squared value reported. When the chi-squared value was not reported and only proportions/percentages are available, we computed the chi-square using the total n for the implicit test or the awareness test (N_participants_implicit and N_participants_awareness, respectively). To compute the n of successes and failures for the chi-squared, we adopted the following convention:

* success = participant displayed the effect/noticed the US
* failure = participant did not display the effect/did not notice the US

We computed the phi statistic, which is equivalent to the r correlation coefficient, using the formulas in Rosenthal and Dimatteo (2001, p. 72) and Navarro (2015, section 12.4). 


### Cohen's dz and drm from t-values

#### Cohen's dz

We computed cohen's dz due to the repeated measures nature of the designs. Cohen's dz is computed using the formula from Rosenthal (1991, p. 15):

d = t/sqrt(df)

We did NOT employ Lakens' equation 7, as this is incorrect (used n instead of dfs).

Whenever necessary, we converted from cohen's d to d using the following equation:

Formula 2 from Morris and DeShon [@Morris2002, p. 111, formula 12], formula from Lakens (https://github.com/Lakens/ANOVA_power_simulation/blob/master/d_to_dz.R):

d_to_dz <- function(d, r){
  dz <- d/(sqrt(2*(1-r)))
  invisible(list(d = d,
                 r = r,
                 dz = dz))
}


#### Cohen's drm
We chose not to employ cohen's drm, following Lakens (2013), who recommends against it. Regardless, our initial approach was to use the formula for computation of drm in Dunlap et al. [-@Dunlap1996] to convert cohen's d to drm.

[insert equations]

Formula 1. from the equations in Dunlap et al. [-@Dunlap1996, p. 171, formulas 2 and 3]:

compute.cohens.drm <- function(cohensd, exp_cor_pairs, exp_n) {
  cohens.drm <- (cohensd * sqrt((2 *(1 - exp_cor_pairs))/exp_n))/(sqrt(2/exp_n))
}

The code previously employed was the following:

```{r}
# Compute Cohen's drm from cohen's d

#cor_pairs_names <- paste("c", effect_indices, "_cor_pairs",
#                                sep = "")

# Function to compute
# compute.cohens.drm <- function(cohensd, r){
#   dz <- cohensd/(sqrt(2*(1-r)))
# }
# 
# 
# for(index in effect_indices) {
#   exp_r_name <- paste("c", index, "_cor_pairs", sep = "")
#   cohensd_name <- paste("c", index, "_cohensd", sep = "")
#   cohensdrm_name <- paste("c", index, "_cohensdrm", sep = "")
#   if(exists(exp_r_name)) {
#     assign(cohensdrm_name, compute.cohens.drm(get(cohensd_name), get(exp_r_name)))
#   } else {
#     assign(cohensdrm_name, compute.cohens.drm(get(cohensd_name), cor_pairs))
#   }
# }

# cohen's drm
# implicit_cohensdrm_names <- paste("c", effect_indices, "_cohensdrm",
#                                 sep = "")
# 
# implicit_cohensdrm <- map_dbl(implicit_cohensdrm_names, get)
# 
# # Compute variance of drm
# compute.variance.drm <- function(cohensdrm, r, n) {
#   (1/n + (cohensdrm)^2/2*n) * 2*(1-r)
# }
# 
# 
# 
# for(index in effect_indices) {
#   exp_r_name <- paste("c", index, "_cor_pairs", sep = "")
#   cohensdrm_name <- paste("c", index, "_cohensdrm", sep = "")
#   varianced_name <- paste("c", index, "_variancedrm", sep = "")
#   n_name <- paste("c", index, "_n", sep = "")
#   if(exists(exp_r_name)) {
#     assign(varianced_name, compute.variance.drm(get(cohensdrm_name), get(exp_r_name), get(n_name)))
#   } else {
#     assign(varianced_name, compute.variance.drm(get(cohensdrm_name), cor_pairs, get(n_name)))
#   }
# }
# 
# 
# # cohen's drm variances
# implicit_variancedrm_names <- paste("c", effect_indices, "_variancedrm",
#                                   sep = "")
# 
# implicit_variancedrm <- map_dbl(implicit_variancedrm_names, get)

#hedges_g(implicit_cohensdrm, implicit_totaln)
```


We opted to standardize all effect sizes in r, due to its advantages. This includes being able to compare distinct designs. According to Rosenthal (1991, p. 17):

"Thus when we employ r as our effect size estimate, we need not make any special adjustment in moving from t tests for independent to those for correlated observations."

### r from t-values or f-values

Formula taken from Rosenthal and Dimatteo (2001, p. 72). This is independent of the design (between or within).

### r from z-values

Formula taken from Rosnow and Rosenthal (2003, p. 231), formula 29. This is independent of the design (between or within).


### Computation of correlations to calculate variance of d for hedge's g

To compute the correlations between pairs of observations, we used the following procedure [@Alexander1990; @Rosenthal2001]:

1. Transform the correlation value to a z score;

2. Compute the weighted average of the z-transformed correlations (by the sample size);

3. Convert the weighted mean z-transformed correlation back to correlation coefficient.

Alternative procedures include [@Field2010].

Three datasets were available to compute the correlation estimate: Razpurker-Apfeld et al. (2004); Schnuerch et al. (2016); and Beanland and Pammer (2010). We computed the correlations between the following conditions:

* Razpurker-Apfeld et al. (2004): correlations between RTs in each target type level (same target vs. different target), across background types (same background and different background) and separately for each grouping condition(columns/rows and triangle arrow);

* Schnuerch et al. (2016): correlation between the mean RTs in the neutral and the incongruent conditions, only for unaware participants;

* Beanland and Pammer (2010): correlations between accuracy in the critical trial and in the averaged control trials, separately for each experiment (1 and 2) and each condition in each experiment (fixating and moving for exp 1, slow and fast for exp. 2).


## Conversion between effect sizes:

* From r to d: The formula presented in [@ellis] as well as in Rosenthal and Dimatteo (2001, p. 71) is:

cohensd = (2 * r)/sqrt(1-(r ** 2))

However, this is the formula for between-subject designs. For within-subject designs, the formula is (inferred from formulas 2.8 and 2.13 of Rosenthal, 1991, p. 15):

cohensd = r/sqrt(1-(r ** 2))

According to Rosenthal (1991, p. 17):

"That is not the situation for equations 2.12 and
2.13, however. When the effect size estimates are the mean differences divided either by S or by u,the definition of the size of the study changes by a factor of 2 in going from t for independent observations to t for correlated
observations."

Thus, we eliminated the 2 multipling the numerator in the between-subjects equation. This formula can also be arrived at by rearranging equations 2.8 and 2.13.

* From d to r: 

The formulas in Rosenthal and Dimatteo (2001, p. 71) are for between-subject designs. Although r can be computed from t and f statistics independently of the design, the relationship between r and d does depend on the design. The formula to compute r from d in within-subject designs is:

r = sqrt(d ** 2/(d **2 + 1))


## Calculate implicit effect sizes by study

### 1. Ariga et al. (2008)

Design: within-subjects


Use reported t-values and dfs to compute ES. 

ES is negative, because it is in the direction opposite of what would be expected if the participant noticed the US (i.e., RTs for invalid same-object are larger then for invalid different-object, rather than smaller).

### 2 to 5. Beanland and Pammer (2011)

Design: within-subjects


Each condition (eyes-fixating x eyes-moving in exp. 1A, slow-US x fast-US in exp. 2) is a separate contrast, so each should be coded in a separated line.

Should we average the mean and sd of accuracy across critical trials (3 and 5) and control trials (2 and 4)? 

First, there is a difference in the degree of awareness: For non-noticers only, this is ok, since trials don't differ in awareness. For noticers, this would be problematic, since this group includes both individuals who noticed the US in the first critical trial and those who didn't.

If we only analyze non-noticers processing, this should not be a problem. Since implicit processing effects are computed from non-noticers only, we can average trials for them without resulting in mixing explicit and implicit effects. This demands that the implicit group be composed only of participants who did not notice the US in either the first or the second critical trial.

There is still the issue of repeated exposition, which differs between the first and second critical trials. In a lot of studies, we have multiple critical trials, in others, only one. Considering that we don't have access to single-trial data in most studies, I propose we do average the critical trials. This can be used later to compare power between studies with distinct numbers of critical trials. To analyze averaged trials, we need the combined means and standard deviations of both control trials averaged and both critical trials averaged. These can be computed from the raw data sent by Beanland.

- Exp. 1A: Table 2 on page 981.

- Exp. 2: Table 8 on page 986.


### 6 and 7. Gabay et al. (2012), exps. 1 and 2

Design: within-subjects

We computed the ES from f-values and dfs.

Experimental condition: valid
Control condition: invalid

* Exp 1 (p. 627): ES is positive, because difference is in the direction expected if the participant processed the US (lower RTs for valid cues).

* Exp 2 (p. 629): ES is positive, because difference is in the direction expected if the participant processed the US (lower RTs for valid cues).


### 8 to 11. Lo and Yeh (2008), exps. 1 and 2

We computed the ES from the reported t-values and dfs. 

Cohen's d is given (0.71); computation of the same statistic from t-values and dfs leads to approximately the same result (0.7074).

* Exp 1, 200ms (p. 1173): ES is positive, because effect is in the expected direction if the US was perceived (percentage of participants above 50% choosing the longer line according to the Ponzo illusion).

* Exp 1, 500ms (p. 1173): ES is positive, because effect is in the expected direction if the US was perceived (percentage of participants above 50% choosing the longer line according to the Ponzo illusion).

* Exp 2, 200ms (p. 1177): ES is positive, because effect is in the expected direction if the 
US elicited a Simon effect (smaller RTs in the compatible condition than in the incompatible condition).

* Exp 2, 500ms (p. 1177): ES is positive, because effect is in the expected direction if the 
US elicited a Simon effect (smaller RTs in the compatible condition than in the incompatible condition).


### 12. & 13. Moore and Egeth, exps. 1 and 3

Experiment 2 is not included because it aims only at eliminating a visual confound; it does not include the illusion.

The effect of the background on the responses is given by the percentage of subjects who responded according to the illusions (Ponzo illusion in exp. 1, Müller-Lyer illusion in exp. 3), compared with chance.

If the percentage is at 50%, it is equal to what is expected if participants were to respond randomly (50% chance upper line, 50% chance lower line). If above 50%, this is evidence that the illusion occurred, raising the percetange. If below 50%, the illusion did not occur and participants correctly noticed that lines are equal.

- Exp. 1: "inattention block" data on page. 345. Effect size is positive, because percentage is above 50%.

- Exp. 3: "inattention block" data on page. 349. Effect size is positive, because percentage is above 50%.


### 14. Moore et al. (2003), exp. 3, p. 313.

The ES was computed from the reported t-value and n of pairs. 

The ES is positive, because it is in the direction expected if the US was processed: the illusory figures interfere with the line judgement when they are superposed on the line path (vertical condition), hence RTs are slower in this condition (compared to the horizontal condition).

### 15. Moore et al. (2004), pp. 713 and 714.

The ES was computed from the reported t-value and n of pairs.

The ES is positive, because it is in the direction expected if the US was processed: the simon effect facilitates responses in the same side as the stimulus when it is consistent with the target, hence RTs are faster in this condition (compared to the inconsistent condition). 

Although the result for IB participants for the inattention trial do not explicitate the direction of the difference, the relative magnitudes are shown in page 714, figure 4B. Additionally, the result for the divided attention condition, when the effect is significant, show a positive value and assert a simon effect; so, a positive value means facilitation according to the Simon Effect, which is what would be expected if the US is processed.

### 16. Most et al. (2005), exps. 1-7 pooled, p. 235

We computed the US using the reported t-value and the n of pairs.

The ES is positive, because it is in the direction expected if the US was processed: accuracy decreases in the critical trial compared to the pre-critical (second) trial.


### 15 to 19. Razpurker-Apfeld et al. (2008)

RT data were computed from the raw data (sent by the researchers).
Each condition (column/row vs. triangle/arrow) is treated as a separate constrast. 

-RT: Implicit processing is indicated by the congruence effect, which is indicated by an interaction (better performance in same trials during same-backgrounds trials and better performance in different trials during different-background trials).

* Column/row grouping: The ES is positive, because the direction of the interaction matches what is expected is the US is processed: faster judgements for at least one of the target x background conditions (here, for target same x background same).

* Triangle/arrow grouping: The ES is negative, because the difference between mean differences is positive (which is the opposite direction of the prediction if the US is processed): differences for "target-same" are as predicted, but the difference for "target-different" is opposite and larger, resulting in a negative value.

- d': Implicit processing is the difference between d' in same and different background conditions. d' = hit - false alarms in different trials. If d' is positive, effect is facilitatory (hits > false alarms), so the effect size is also positive; otherwise, the effect size is negative.

* Column/row grouping: The ES is positive, because the direction of the interaction matches what is expected is the US is processed: faster judgements for at least one of the target x background conditions (here, for target same x background same).

* Triangle/arrow grouping: not enough info is reported to compute the ES.


### 20. & 21. Richards et al. (2012)

Data comes from page 174 in the paper, on Table 1, only from "IBs" participants. Lines from the inspection screen stage are not included because this stage does not include the US (red cross). Two options for effect:

 - Number of participants fixating on the US is above chance; or
 
 - Difference in fixations and in gaze time between IBs who fixate the US and those who don't.
 
We can also split the sample between subsamples, each forming a contrast.

- Exclude from meta-analysis - not behavioral
 
 
### 22. to 26. Russel and Driver (2005)

Similar to Razpurker-Apfeld et al. (2008).

Exp. 1, acc: interaction significant, positive sign because effect is in predicted direction

Exp. 1, RT: interaction non-significant, positive sign because effect is in predicted direction

Exp. 2, acc: interaction significant, positive sign because effect is in predicted direction (negative total difference, because difference for dt is predicted and larger than unpredicted difference for st)

Exp. 2, RT: interaction non-significant, negative sign because effect is in oppposite direction to predicted (positive total difference, because difference for st is nonpredicted and larger than predicted difference for dt)

Exp. 3, acc: interaction significant, positive sign because effect is in predicted direction (negative total difference, because difference for dt is predicted and larger than unpredicted difference for st)

Exp. 3, RT: interaction non-significant, negative sign because effect is in oppposite direction to predicted (positive total difference, because difference for st is nonpredicted and and difference in dt is also nonpredicted difference)

Exp. 4A, acc: interaction significant, positive sign because effect is in predicted direction (negative total difference, because difference for dt is predicted and difference for st is also predicted)

Exp. 4A, RT: interaction non-significant, but data not available to compute effect size

Exp. 4A, acc: interaction significant, positive sign because effect is in predicted direction (negative total difference, because difference for dt is predicted and larger than null difference for st)

Exp. 4B, RT: interaction non-significant, but data not available to compute effect size

Exp. 5, acc: interaction significant, positive sign because effect is in predicted direction (negative total difference, because difference for dt is predicted and difference for st is also predicted)

Exp. 5, RT: interaction non-significant, positive sign because effect is in predicted direction (negative total difference, because difference for dt is predicted and difference for st is also predicted)



### 28. & 29. Schnuerch et al. (2016) exps. 1 and 2.

Difference between incongruent and neutral trials in RT, which tests the hypothesis. Both effects are significant and facilitatory, thus have a positive sign.

Computed Cohen's d in experiment 1 is different from reported Cohen's d (0.43). Keep computed and register in paper.


### 30. Scholte et al. (2006)

There are multiple contrasts, for v1, v2, v3, and v4/v8. Should all of them be separated?

- Exclude from meta-analysis - not behavioral

### 31. Vandenbroucke et al. (2014) - DO NOT USE (fMRI)

- Exclude from meta-analysis - not behavioral

### 32. Wiemer et al. (2013) - DO NOT USE (SCR)

Difference between SCRs evoked by flower stimuli and spider stimuli (p.161). The effect is facilitatory - thus, the sign is positive - and significant.

Design in between-subjects.

Degree of freedom is decimal?? Round down?

- Exclude from meta-analsis - not behavioral


### 33 to 35. Mack and Rock's experiments from chapter 8

#### Exp. 1 - p. 178-179

Use reported number of subjects who offered prime word as stem-completion in the priming experiment and in the control experiment; build a contingency table with the proportions; and compute chi-squared from the contingency table.

In figure 1, reported percentage of participants who offered the words as stem completion does not match the n reported in the text on page 179; using percentages to compute proportions instead.

#### Exp. 2 - p. 182

Use reported number of subjects who offered prime word as stem-completion in the priming experiment and in the control experiment; build a contingency table with the proportions; and compute chi-squared from the contingency table.

#### Exp. 3 - p. 184

Use reported number of subjects who offered prime word as stem-completion in the priming experiment and those in the control experiment who were presented roots for the same words as in exp 3 (short and flake) shown in figure 1 (p. 178), to control for stimulus properties like frequency; build a contingency table with the proportions; and compute chi-squared from the contingency table.

In figure 1, reported percentage of participants who offered "short" as stem completion + those who offered "flake" does not match the n reported in the text on page 179; using percentages to compute proportions instead.

#### Exp. 4 - p. 187-188

Use reported number of subjects who offered prime word as stem-completion in the priming experiment and in the control experiment; build a contingency table with the proportions; and compute chi-squared from the contingency table.

Reported chi-square value was incorrect (14.54, p.188).

#### Exp. 5 - p. 191

Use reported number of subjects who offered prime word as stem-completion in the priming experiment and in the control experiment; build a contingency table with the proportions; and compute chi-squared from the contingency table.

### 36. Wood and Simons (2019), exps. 1 and 2

Use data from "lax" criterion to reduce likelihood of false implicit results

Use reported percentage of participants who chose the longer line (exp. 1) or who completed the word-stem with the prime (exp. 2) to compute ES.

* Exp 1, p. 5 of pdf: ES is positive, because effect is in the direction of what is expected if the US was processed: participants chose the longer line above chance.

* Exp 2, p. 9 of pdf: ES is negative, because effect is in the opposite direction of what is expected if the US was processed: very few participants (below chance) completed the word-stem with the prime word.


### 46 to 54. Rashal et al. (2017)

* Exp. 1: data for acc results was not reported.

* Exp. 2: data for RT results was not reported. ES is negative, because the effect is in the opposite direction of what is expected is the US was processed.

* Exp. 3: data for acc results was not reported.



### 55 to 60. Kimchi et al. (2004)

Effect sizes from f-values and dfs.

Each grouping condition is treated as a separate constrast. 


* Column/row by color similarity: The ES is positive, because the direction of the interaction matches what is expected is the US is processed: faster judgements for at least one of the target x background conditions (here, for target same x background same). Data for accuracy results was not reported.

* Triangle/arrow by color similarity: data for RT results was not reported.

* Triangle/arrow grouping: The ES is negative, because the difference between mean differences is positive (which is the opposite direction of the prediction if the US is processed): differences for "target-same" are as predicted, but the difference for "target-different" is opposite and larger, resulting in a negative value. Data for RT results was not reported.

- d': Implicit processing is the difference between d' in same and different background conditions. d' = hit - false alarms in different trials. If d' is positive, effect is facilitatory (hits > false alarms), so the effect size is also positive; otherwise, the effect size is negative.

* Column/row grouping: The ES is positive, because the direction of the interaction matches what is expected is the US is processed: faster judgements for at least one of the target x background conditions (here, for target same x background same).

* Triangle/arrow grouping: not enough info is reported to compute the ES.

* Connected triangle/arrow: The ES is positive, because the direction of the interaction matches what is expected is the US is processed: faster judgements for at least one of the target x background conditions (here, both for target same x background same and for target different x background different).

* Square/cross by color similarity: enough info to compute effect size only for accuracy. The ES is negative, because the difference is in the opposite direction to what is predicted for both the same and different conditions. Data for RT results was not reported.

* Square/cross: The ES is positive, because the direction of the differences is in the predicted direction for both RT and accuracy, both in the same and different conditions.

Vertical/horizontal line by color similarity: not enough info is reported to compute the ES.

Disconnected square/cross: The ES is positive, because the direction of the differences is in the predicted direction for both RT and accuracy, both in the same and different conditions.


### 72. Pugnaghi et al. (2020)

Effect sizes from f-values and dfs.

Use f-value instead of collapsing t-values across low-load and high-load conditions, because the latter loses info about the variance between load conditions.


### 76. Nobre et al. (2020)

Data only for RT (accuracy is not comparable due to the adaptive procedure)

No correlation between conditions (square/random), because trials in each condition are not necessarily equivalent due to the adaptive procedure.


### 77. Pugnaghi et al. (2019) - p. 8

Use the data for analysis of presence vs. absence of US, instead of data for repeated vs new stimuli, to investigate implicit processing of the presence of unexpected stimuli instead of types of implicit processing (point made by reviewer 1, point 1.4).

Effect sizes from t-values and dfs. These are reported in the discussion instead of the results section.


### 78 to 91. Kreit et al. (2020) - datasets

The data reported for implicit effects are related to performance in the questions originally intended to assess awareness in the original studies. This creates an issue in that, if selection of IB participants in the review is done using the same criteria as in the original studies, the same measure is used both for selection of participants and for assessment of implicit processing. This is not explicitly stated in the text; they only mention that participants were included who "did not notice the unexpected stimulus".

The solution is to compare the n included for each study in the original study and in the review. If this differs, this means that the authors changed the participant selection criteria in the review.

For :

 ####1 - "Kreitz et al. (2015) - Does semantic preactivation reduce inattentional blindness"
 ####2 - "Kreitz et al. (2015) - Inattentional Blindness and Individual Differences in Cognitive Abilities"
 ####3 - "Kreitz et al. (2015) - Some See It, Some Don’t: Exploring the Relation between Inattentional Blindness and Personality Factors"
 ####4 - "Kreitz et al. (2015) - Inattentional blindness is influenced by exposure time not motion speed"
 ####5 - "Kreitz et al. (2016) - The Influence of Attention Set, Working Memory Capacity, and Expectations on Inattentional Blindness"
 
only percentages are reported (2) or depicted in figures (1,4). For (3), the n-included reported in the original paper is 354, whereas in the review it is 290. Thus, the criterion has changed for that study.

For (5), the number is the same (available in Table 2 of the original study), indicating that the same tasks are used to select the sample and assess performance, which is problematic.

An alternative is to compare performance in critical and control trials in those experiments. However, the data for the review paper does not include performance in the control trials, only in the critical trial. Thus, we need the raw data for the individual papers. Raw data is available for the following studies:


# Calculate awareness effect sizes by study

For this analysis, only studies that use "group assessment of awareness" are included. The rationale is the following: the interest of this analysis is to assess how likely is, meta-analytically, that participants noticed the US more often than chance. Studies that did not employ group assessment of awareness ("post-hoc selection studies") do not allow this because they selected participants individually. Because the interest of this analysis is on how the classification of participants as aware or unaware may influence the results on implicit processing, we include in this analysis only studies which employ “group assessment of awareness".

Moreover, using a rate of aware participants for post-hoc selection means including aware participants in the computation, even though they were not included in the final analysis for implicit processing. Conversely, not including those would always lead to a 0% rate of aware participants for those studies. So they should not be included.

By convention, we established that an ES in the direction of aware is positive, and an ES in the direction of unaware is negative. 50% is chance.

## Proportions or percentages

When an awareness ES is computed from proportions or percentages:

* success = participant was aware
* failure = participant was unaware

For studies that report n or proportions/percentages of noticers and/or perform a goodness-of-fit test (chi-square, or fisher's exact test) to test against the hypothesis that the n or the proportion/percentages of participants is different from what would be expected by chance, we used the following conventions:

* If the percentage of noticers is at 50%, it is equal to what is expected if participants were to respond randomly (50% noticers, 50% nonnoticers). 

* If above 50%, this is evidence that the participants noticed the US and were thus aware. In this case, the ES is positive. 

* If below 50%, participants did not notice the US and did not respond randomly; instead, they asserting that they saw nothing (were not aware). In this case, the US is negative.


### 1. Ariga et al. (2008)

We use the reported percentage of noticers. The study reports percentages of noticing for two forced-choice questions. We use the higher percentage of the two, to avoid underestimating awareness.

Criterion for chance: 50%, as both questions are dichotomous.


### 2 to 5. Beanland and Pammer (2011)

We use the reported percentage of noticers.

Since for the implicit processing effect sizes we considered only participants who were IB in both trials, and the goal of using awareness effect sizes is to assess the power of awareness measures, we adopted a conservative threshold and considered a success any participant who notices the US in either the first or the second critical trial.

* Exp. 1A, fixating: p. 980; effect size is negative, because percentage is below 50%;
* Exp. 1A, moving: p. 980; effect size is negative, because percentage is below 50%;
* Exp. 2, slow US: p. 985; effect size is negative, because percentage is below 50%;
* Exp. 2, fast US: p. 985; effect size is negative, because percentage is below 50%.

# 6 and 7. Gabay et al. (2012), exps. 1 and 2

We used the reported n of noticers.

* Exp. 1, p. 627; effect size is negative, because n is below 50% of sample.
* Exp. 1, p. 628; effect size is positive, because n is above 50% of sample.


### 8 to 11. Lo and Yeh (2008), exps. 1 and 2

We computed the ES from the reported chi-squared values from the v-shape discrimination task, because 1) the results from the confidence ratings did not offer a result against chance reports; and 2) the study classified participants as aware or unaware bases on the discrimination task instead of the confidence ratings.

Criterion for chance: 50%, as the v-shape question is dichotomous.

* Exp 1, 200ms (p. 1172): ES is positive, because percentage is above 50%;

* Exp 1, 500ms (p. 1172): ES is positive, because percentage is above 50%;

* Exp 2, 200ms (p. 1176): ES is negative, because percentage is below 50%;

* Exp 2, 500ms (p. 1177): ES is negative, because percentage is below 50%.


### 12 and 13. Moore and Egeth, exps. 1 and 3, 

Effect sizes computed using percentage of noticers reported. We use percentages of direct query instead of forced-choice, since forced-choice can be performed using implicit knowledge.

Criterion for chance: 50%, as the direct query has a dichotomous answer (yes/no).

* Exp. 1, p. 345: effect size is negative, because percentage is below 50%;
* Exp. 3, p. 349: effect size is negative, because percentage is below 50%.

Result for exp. 3 is infinite (0 participants noticed the pattern), so replace with largest effect in data.


### 14. Moore et al. (2003), exp. 3 - p. 312

Use the reported percentage of noticers to compute ES. We used the percentage reported for the question about regularity (p. 312), instead of orientation (p. 313), since the former assesses perception more generally and thus might be more sensitive.

Criterion for chance: 50%, as the question about regularity has a dichotomous answer (regular/irregular).

Effect size is positive, because percentage reported is above 50%.


### 15. Moore et al. (2004), p. 712.

We use the reported percentage of noticers to compute ES. We used the percentage reported of direct query (712) instead of forced-choice for awareness effect, since this can be performed using implicit knowledge.

Criterion for chance: 50%, as the direct query has a dichotomous answer (yes/no).

The ES is negative, because the percentage reported is below 50%.


### 15. Most et al. (2004)

Effect size is computed from the pooled reported n or percentage of noticers across experiments.

The ES is negative, because the resulting proportion of noticers is above 50%.

#### Exp. 1 - p. 228

Use reported sum of number of noticers across conditions as successes and total n across conditions as population. Although this mixes distinct rates of noticing in distinct conditions, this is coherent with how the implicit effect sizes were reported in experiment eight, where conditions are also mixed.

#### Exp. 2 - p. 229

Use reported percentage of noticers in each condition to compute n of noticers, and sum results to obtain n of successes.

#### Exp. 3 - p. 229 (rates), 230 (total n)

Use reported percentage of noticers in each condition to compute n of noticers, and sum results to obtain n of successes.

#### Exp. 4 - p. 229 (rates), 231 (total n)

Use reported percentage of noticers in each condition to compute n of noticers, and sum results to obtain n of successes.

#### Exp. 5 - p. 229 (rates), 233 (total n)

Use reported percentage of noticers in each condition to compute n of noticers, and sum results to obtain n of successes.

#### Exp. 6 - p. 229 (rates), 233 (total n)

Use reported percentage of noticers in each condition to compute n of noticers, and sum results to obtain n of successes.

#### Exp. 7 - p. 229 (rates), 234 (total n)

Use reported percentage of noticers in each condition to compute n of noticers, and sum results to obtain n of successes.


### 16. Razpurker-Apfeld et al. (2008) 

Use reported number of noticers (p. 188) and total n (p. 185) to compute chi-square. The number of noticers used is that from the "organization" question, because this is the one used by the authors to infer unawareness.

Criterion for chance: ???? open question.

Two distinct effect sizes are computed by condition; within the same condition, the effect size is the same for RT and d', since it is the same sample. They are not coded as separate ES, even though RT and d' are separate entries (lines) for the implicit ES, because they must not contribute twice to the model.

* Columns/rows condition, p. 188: ES is negative, because of of noticers is below 50%.
* Triangle/arrow condition, p. 188: ES is negative, because of of noticers is below 50%.


### 17. Richards et al. (2012), p. 173. - DO OT USE (EYE TRACKING)

Use reported percentage of noticers to compute n of noticers and ES.

ES is negative, because percentage reported is below 50%.


### 18 to 33. Russel and Driver (2005), exps. 1-5

#### Exp. 1 - p. 610

Use reported n of noticers to compute chi-squared.

Criterion for chance: 50%, as all questions are dichotomous.

ES is negative, because n of noticers is below 50%.

#### Exp. 2 - p. 611

Use reported n of noticers to compute chi-squared.

ES is positive, because n of noticers is above 50%.

#### Exp. 3 - p. 613

Use reported n of noticers to compute chi-squared.

ES is 0.

#### Exp. 4A - p. 614-615

Only a single yes-or-no question; use as both lax and strict.

Use reported n of noticers to compute chi-squared.

ES is negative, because n of noticers is below 50%.

#### Exp. 4B - p. 615

Only a single yes-or-no question; use as both lax and strict.

Use reported n of noticers to compute chi-squared.

ES is negative, because n of noticers is below 50%.

#### Exp. 5 - p. 618

Lax: question 1 (yes-or-no question)

Strict: question 2 (2AFC for rows or columns)

Use reported n of noticers to compute chi-squared.

ES is positive, because n of noticers is above 50%.

### 35 and 36. Schnuerch et al. (2016), exps. 1 and 2.

We computed the ES from the percentage of noticers,  which is both experiments was obtained by the reported percentage of nonnoticers.

* Exp 1, p. 3: the ES is negative, because the percentage of noticers is below 50%.
* Exp 2, p. 4: the ES is negative, because the percentage of noticers is below 50%.


### 38 and 39. Wood and Simons (2019), exps. 1 and 2

We use the reported percentage of noticers for the lax criterion. This is important to avoid understimating awareness.

* Exp 1: p. 5 (of pdf); effect size is negative, because percentage is below 50%.

* Exp 2: p. 9 (of pdf); effect size is positive, because percentage is above 50%.


### 41 to 45. Mack and Rock's experiments from chapter 8

#### Exp. 1 - p. 178

Use reported number of unaware subjects to compute proportions.

ES is negative, because n of noticers is below 50%.

#### Exp. 2 - p. 182

Use reported number of unaware subjects to compute proportions.

ES is negative, because n of noticers is below 50%.

#### Exp. 3 - p. 184

Use reported number of unaware subjects to compute proportions.

ES is negative, because n of noticers is below 50%.

#### Exp. 4 - p. 188

Use reported number of unaware subjects to compute proportions.

ES is positive, because n of noticers is above 50%.

#### Exp. 5 - p. 191

Use reported number of unaware subjects to compute proportions.

ES is positive, because n of noticers is above 50%.


### 46 to 54. Rashal et al. (2017)

Use reported number of noticers for change question. Reasons:

* It does not demand identification;
* It is more similar to yes-or-no questions, and the "lax" criterion of Wood and Simons (2019), so we assume it is less likely to underestimate awareness.

* exp. 1 - p. 2078, table 1; effect size is positive, because percentage is above 50%
* exp. 2- p. 2078, table 1; effect size is positive, because percentage is above 50%
* exp. 3 - p. 2082, table 2; effect size is positive, because percentage is above 50%
* exp. 4 - p. 2082, table 2; effect size is negative, because percentage is below 50%
* exp. 5 - p. 2082, table 2; effect size is 0.
* exp. 6 - p. 2082, table 2; effect size is positive, because percentage is above 50%

Compare computed chi-squares with reported chi-square: differences found for experiments 3 and 4.

### 57 to 69, 68 to 71. Kimchi et al. (2004)

Lax:

Use reported number of noticers for "change" question (column "something" in Table 1). Reasons:

* It does not demand identification;
* It is more similar to yes-or-no questions, and the "lax" criterion of Wood and Simons (2019), so we assume it is less likely to underestimate awareness.

Strict: 

* It demands the most complex identification compared to the color and "something";
* It is more similar to yes-or-no questions.

Use reported number of noticers for "pattern" question (column "organization" in Table 1). Reasons:

* exp. 1, connected triangle/arrow - p. 692, table 1; effect size is positive, because percentage is above 50%
* exp. 2, disconnected square/cross - p. 695, table 2; effect size is positive, because percentage is above 50%


Compare computed chi-squares with reported chi-square: differences found for experiments 3 and 4.


### 72 to 75. Pugnaghi et al. (2020) - Exps. 1 and 2

Lax awareness effect sizes would be only responses to the yes-or-no question; strict would be responses to both questions. Only strict effect sizes area available in the paper and datasets.


### 76. Nobre et al. (2020)




### 77. Pugnaghi et al. (2019)

None of the experiments report detailed responses to the assessment of awareness. We use as successes only the classifications provided by the authors (noticers and non-noticers) in the raw data excel file.

Noticers may have met one or more of the others exclusion criteria: check each noticer's row in the excel file for this. Participantes who meet those other criteria should not be counted.


* Exp. 1: 1 noticer, no exclusion criterion.
* Exp. 2: 1 noticer, no exclusion criterion; 1 noticer with exclusion criterion.
* Exp. 3: 9 noticers, no exclusion criterion.

Use total across experiments, as results for implicit processing are reported for the 3 experiments pooled only.

Total: 1 + 1 + 9 = 11.


### 78 to 91. Kreit et al. (2020) - datasets

Retrieve n of noticers from raw data or from difference between N_participants_awareness and nincluded in the paper.

Noticers may have met one or more of the others exclusion criteria: check each noticer's row in the excel file for this. Participantes who meet those other criteria should not be counted.

* Dataset 1: 47 noticers with no exclusion criteria; 1 noticer with exclusion criterion.
* Dataset 2: difference between N_participants_awareness and nincluded in the paper: 172-86=86.
* Dataset 3: difference between N_participants_awareness and nincluded in the paper: 111-62=49.
* Dataset 4: difference between N_participants_awareness and nincluded in the paper: 110-64=46.
* Dataset 5: difference between N_participants_awareness and nincluded in the paper: 106-38=68.
* Dataset 6: difference between N_participants_awareness and nincluded in the paper: 554-290=264.
* Dataset 7: difference between N_participants_awareness and nincluded in the paper: 95-42=53.
* Dataset 8: difference between N_participants_awareness and nincluded in the paper: 90-33=57.
* Dataset 9: difference between N_participants_awareness and nincluded in the paper: 89-21=68.
* Dataset 10: difference between N_participants_awareness and nincluded in the paper: 115-34=81.
* Dataset 13: difference between N_participants_awareness and nincluded in the paper: 188-57=131.
* Dataset 14: difference between N_participants_awareness and nincluded in the paper: 184-64=120.
* Dataset 15: difference between N_participants_awareness and nincluded in the paper: 277-68=209.
* Dataset 16: difference between N_participants_awareness and nincluded in the paper: 260-178=82.

### 19. Shafto and Pitts (2015) - p. 10943 - DO NOT USE (EEG)

Use reported number of inattentionally blind subjects.

### 20. Wiemer et al. (2013) - p. 160 - DO NOT USE (SRC)

Use of percentage of noticers by condition to compute proportions, sum across conditions and compute chi-squared.


# Meta-analytic model fitting

## Implicit model:

### Moderation

We chose to run a mixed effect subgroup analysis (fixed-effect between subgroups, random-effect within subgroups) because we assume the moderators are exhaustive levels of the studies [@Harrer].
(https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/subgroup.html).

This perform test both within subgroups and then comparisons between subgroups.


The following variables to conduct moderation analysis so far for the implicit effects:

* Measure: RT vs accuracy

* Relevance: irrelevant vs. relevant

* N of trials - this needs to be done with a regression, using n of trials as a continuous predictor

* Inattention paradigm

* Group assessment of awareness instead of inattetion paradigm


### Heterogeneity

* Visual inspection of forest plot: Wood and Simons (2019), exp. 2, has a disproportionately wide confidence interval.

* Heterogeneity test: low values of tau^2 and I^2. The test showed no significant results.

* No outliers were detected. The confidence interval of Wood and Simons (2019), exp. 2, is not out of bounds.

* Influence analysis: two inattention studies (Rashal et al., 2017; Kimchi and Razpurker-Apfeld, 2004) and a non-inattention study (Gabay et al., 2012) contribute largely for heterogeneity, while having a small influence in the pooled effect size. Wood and Simons (2019), exp. 2, has disproportionately wide confidence intervals and a huge influence in the effect size.

Fitting the model without those studies did not change the qualitative results. However, Wood and Simons (2019) exp 2 was removed both due to:

* The influence in the pooled ES
* The impossibility of calculating base rates for choosing the words used as primes, in virtue of the absence of a control study.


## Awareness model

# Publication bias

We did not perform analysis of publication bias, as the reports of awareness measures do not seem to be reported in a way as to avoid publication of negative results.  

## p-curve

We cannot conduct a p-curve analysis, because several of the comparisons are secondary analysis.


# References
